# Tiny LLM zh

## 1.ç®€ä»‹

æœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªå°å‚æ•°é‡çš„ä¸­æ–‡è¯­è¨€å¤§æ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿå…¥é—¨å­¦ä¹ å¤§æ¨¡å‹ç›¸å…³çŸ¥è¯†ï¼Œå¦‚æœæ­¤é¡¹ç›®å¯¹ä½ æœ‰ç”¨ï¼Œå¯ä»¥ç‚¹ä¸€ä¸‹startï¼Œè°¢è°¢ï¼

æ¨¡å‹æ¶æ„ï¼šæ•´ä½“æ¨¡å‹æ¶æ„é‡‡ç”¨å¼€æºé€šç”¨æ¶æ„ï¼ŒåŒ…æ‹¬ï¼šRMSNormï¼ŒRoPEï¼ŒMHAç­‰

å®ç°ç»†èŠ‚ï¼šå®ç°å¤§æ¨¡å‹ä¸¤é˜¶æ®µè®­ç»ƒåŠåç»­äººç±»å¯¹é½ï¼Œå³ï¼šåˆ†è¯(Tokenizer) -> é¢„è®­ç»ƒ(PTM) -> æŒ‡ä»¤å¾®è°ƒ(SFT) -> äººç±»å¯¹é½(RLHF, DPO) -> æµ‹è¯„ -> é‡åŒ– -> éƒ¨ç½²ã€‚

é¡¹ç›®å·²éƒ¨ç½²ï¼Œå¯ä»¥åœ¨å¦‚ä¸‹ç½‘ç«™ä¸Šä½“éªŒã€‚

- [ModeScope Tiny LLM](https://www.modelscope.cn/studios/wdndev/tiny_llm_92m_demo/summary)

é¡¹ç›®ç‰¹ç‚¹ï¼š

- å…¬å¼€å…¨éƒ¨æ•°æ®åŠä»£ç ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®ï¼Œtokenizerç­‰ï¼›ï¼ˆ[Tiny LLM Datasets](doc/datasets_download.md)ï¼‰
- èµ°é€šå¤§æ¨¡å‹æ•´ä¸ªæµç¨‹ï¼šåˆ†è¯(Tokenizer) -> é¢„è®­ç»ƒ(PTM) -> æŒ‡ä»¤å¾®è°ƒ(SFT) -> äººç±»å¯¹é½(RLHF, DPO) -> æµ‹è¯„ -> éƒ¨ç½²ï¼›
- å…¬å¼€é¢„è®­ç»ƒtoken 42Bï¼ŒSFTæ•°æ®400wæ¡ï¼ŒRLæ•°æ® 17wæ¡ï¼›
- è®­ç»ƒ Tokenizerï¼š10G ä¸­æ–‡ç™¾ç§‘æ–‡æœ¬è®­ç»ƒ 20K ä¸­æ–‡è¯è¡¨ï¼Œä¸ Llama2 è¯è¡¨åˆå¹¶ï¼Œæ„å»ºTiny LLMè¯è¡¨ï¼›
- ä½¿ç”¨ Transformers deepspeed è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒå¤šæœºå¤šå¡ï¼Œæ”¯æŒ Zero ç­‰ä¼˜åŒ–æŠ€æœ¯ï¼›
- æ‰€æœ‰ä»£ç  `Bash` è„šæœ¬å¯åŠ¨ï¼Œæ”¯æŒä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œå¦‚16m, 42m, 92m, 210m, 440mç­‰ï¼›
- æ”¯æŒ MoE æ¶æ„ï¼Œåœ¨ [tiny_llm_moe](https://github.com/wdndev/tiny-llm-zh/tree/tiny_llm_moe) æ”¯æŒæœ€æ–°å…±äº«ä¸“å®¶ï¼Œå¹³è¡¡ä¸“å®¶ç­‰æŠ€æœ¯ï¼›
- æ”¯æŒ vLLM æ¨ç†æ¡†æ¶ï¼›
- æ”¯æŒ llama.cpp æ¨ç†æ¡†æ¶ï¼›


æœ¬é¡¹ç›®ä¸»è¦æœ‰ä¸‰ä¸ªåˆ†æ”¯ï¼Œæ¨èå­¦ä¹  ä¸»åˆ†æ”¯ï¼Œå…·ä½“åŒºåˆ«å¦‚ä¸‹ï¼š

- [llama2_torch](https://github.com/wdndev/tiny-llm-zh/tree/llama2_torch) ï¼š æ¨¡å‹æ¶æ„é‡‡ç”¨åŸç‰ˆ Llama2 æ¶æ„ï¼Œåªæ˜¯å°†éƒ¨åˆ†çš„è¾“å…¥è¾“å‡ºä¿®æ”¹ä¸ºé€‚åˆè®­ç»ƒçš„æ ¼å¼ï¼›
- `main`   `tiny_llm` ï¼š å¯¹é½å¼€æºç¤¾åŒºæ¨¡å‹ï¼Œä½¿ç”¨Transformersåº“æ„å»ºåº•å±‚æ¨¡å‹ï¼Œä¹Ÿä½¿ç”¨Transformersåº“è¿›è¡Œå¤šå¡å¤šæœºè®­ç»ƒï¼›
- [tiny_llm_moe](https://github.com/wdndev/tiny-llm-zh/tree/tiny_llm_moe) ï¼š åœ¨`tiny_llm`çš„åŸºç¡€ä¸Šï¼Œä¿®æ”¹ `MLP`å±‚ä¸ºMoEæ¨¡å‹ï¼Œä½¿ç”¨Transformersåº“è¿›è¡Œå¤šå¡å¤šæœºè®­ç»ƒã€‚

æ³¨æ„ï¼š

1. å› èµ„æºé™åˆ¶ï¼Œæœ¬é¡¹ç›®çš„ç¬¬ä¸€è¦åŠ¡æ˜¯èµ°é€šå¤§æ¨¡å‹æ•´ä¸ªæµç¨‹ï¼Œè€Œä¸æ˜¯è°ƒæ•™æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Œæ•…è¯„æµ‹ç»“æœåˆ†æ•°è¾ƒä½ï¼Œéƒ¨åˆ†ç”Ÿæˆé”™è¯¯ã€‚
2. è¯¦ç»†çš„æ•°æ®å¤„ç†ï¼Œè®­ç»ƒè¿‡ç¨‹è§ `doc` æ–‡ä»¶å¤¹ï¼ˆæ­£åœ¨æ•´ç†ã€‚ã€‚ã€‚ï¼‰


## 2.å¿«é€Ÿå¼€å§‹

æ¨¡å‹å·²æ‰˜ç®¡åœ¨ [Huggingface](https://huggingface.co/wdndev/tiny_llm_sft_92m) å’Œ [ModeScope](https://www.modelscope.cn/models/wdndev/tiny_llm_sft_92m) ä¸­ï¼Œå¯è¿è¡Œä»£ç è‡ªåŠ¨ä¸‹è½½ã€‚

å»ºè®®ä½¿ç”¨ Huggingface åœ¨çº¿åŠ è½½æ¨¡å‹ï¼Œå¦‚æœè¿è¡Œä¸äº†ï¼Œåœ¨è¯• ModeScope ï¼›å¦‚æœéœ€è¦æœ¬åœ°è¿è¡Œï¼Œä¿®æ”¹`model_id`ä¸­çš„è·¯å¾„ä¸ºæœ¬åœ°ç›®å½•ï¼Œå³å¯è¿è¡Œã€‚

#### ä¾èµ–å®‰è£…

- python 3.8 and above
- pytorch 2.0 and above
- transformers 4.37.2 and above
- CUDA 11.4 and above are recommended. (if training)

```bash
pip install -r requirements.txt 
```


#### ğŸ¤— HuggingFace

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.generation import GenerationConfig

model_id = "wdndev/tiny_llm_sft_92m"

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", trust_remote_code=True)
generation_config = GenerationConfig.from_pretrained(model_id, trust_remote_code=True)
sys_text = "ä½ æ˜¯ç”±wdndevå¼€å‘çš„ä¸ªäººåŠ©æ‰‹ã€‚"
# user_text = "ä¸–ç•Œä¸Šæœ€å¤§çš„åŠ¨ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ"
# user_text = "ä»‹ç»ä¸€ä¸‹åˆ˜å¾·åã€‚"
user_text = "ä»‹ç»ä¸€ä¸‹ä¸­å›½ã€‚"
input_txt = "\n".join(["<|system|>", sys_text.strip(), 
                        "<|user|>", user_text.strip(), 
                        "<|assistant|>"]).strip() + "\n"

generation_config.max_new_tokens = 200
model_inputs = tokenizer(input_txt, return_tensors="pt").to(model.device)
generated_ids = model.generate(model_inputs.input_ids, generation_config=generation_config)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

#### ğŸ¤– ModeScope

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_id = "wdndev/tiny_llm_sft_92m"

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", trust_remote_code=True)

sys_text = "ä½ æ˜¯ç”±wdndevå¼€å‘çš„ä¸ªäººåŠ©æ‰‹ã€‚"
# user_text = "ä¸–ç•Œä¸Šæœ€å¤§çš„åŠ¨ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ"
# user_text = "ä»‹ç»ä¸€ä¸‹åˆ˜å¾·åã€‚"
user_text = "ä»‹ç»ä¸€ä¸‹ä¸­å›½ã€‚"
input_txt = "\n".join(["<|system|>", sys_text.strip(), 
                        "<|user|>", user_text.strip(), 
                        "<|assistant|>"]).strip() + "\n"

model_inputs = tokenizer(input_txt, return_tensors="pt").to(model.device)
generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=200)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```


ç”Ÿæˆæ•ˆæœ
```bash
é—®ï¼šä¸–ç•Œä¸Šæœ€å¤§çš„åŠ¨ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ
ç­”ï¼šç›®å‰å·²çŸ¥æœ€å¤§çš„åŠ¨ç‰©æ˜¯è“é²¸ï¼ˆBalaenoptera musculusï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåºå¤§çš„å“ºä¹³åŠ¨ç‰©ï¼Œå±äºé¡»é²¸äºšç›®ã€é¡»é²¸ç§‘ä¸­çš„æœ€å¤§ç‰©ç§ã€‚è“é²¸çš„èº«é•¿å¯è¾¾30ç±³ä»¥ä¸Šï¼Œä½“é‡å¯è¾¾175å¨ã€‚å®ƒä»¬åœ¨æµ·æ´‹ä¸­ç”Ÿæ´»ï¼Œä¸»è¦ä»¥æµ®æ¸¸ç”Ÿç‰©ä¸ºé£Ÿï¼Œå¦‚ç”²å£³ç±»åŠ¨ç‰©å’Œå°å‹é±¼ç±»ç­‰ã€‚ç”±äºå…¶å·¨å¤§çš„ä½“å‹å’Œå¤æ‚çš„ç”Ÿæ€ç¾¤è½ï¼Œè“é²¸æˆä¸ºæµ·æ´‹æ—…æ¸¸çš„çƒ­é—¨æ™¯ç‚¹ä¹‹ä¸€ã€‚

é—®ï¼šä»‹ç»ä¸€ä¸‹åˆ˜å¾·åã€‚
ç­”ï¼šåˆ˜å¾·åæ˜¯ä¸€ä½é¦™æ¸¯æµè¡Œæ­Œæ‰‹ã€æ¼”å‘˜å’Œå¯¼æ¼”ï¼Œä»–åœ¨éŸ³ä¹ç•Œçš„è´¡çŒ®éå¸¸å·¨å¤§ã€‚ä»–æ˜¯åè¯­ä¹å›å†å²ä¸Šæœ€ä¼Ÿå¤§çš„è‰ºäººä¹‹ä¸€ï¼Œä»£è¡¨ä½œå“åŒ…æ‹¬ã€Šçˆ±æˆ‘èº«ä½“ã€‹å’Œã€Šè‚¥çš‚æ³¡ã€‹ã€‚ä»–ä¹Ÿç»å¸¸å‚æ¼”ç”µå½±å’Œç”µè§†å‰§ï¼Œå¹¶åœ¨ç”µè§†ä¸Šå—åˆ°å¥½è¯„ã€‚

é—®ï¼šä»‹ç»ä¸€ä¸‹ä¸­å›½ã€‚
ç­”ï¼šä¸­å›½æ˜¯ä½äºä¸œäºšçš„å¤§é™†ï¼Œè¢«æ¬§æ´²ä»¥åŠäºšæ´²å’Œå…¶ä»–å¤§é™†æ‰€åŒ…å›´ã€‚å®ƒæ˜¯ä¸­å›½ç¬¬äºŒå¤§æ–‡æ˜å’Œä¸–ç•Œä¸Šæœ€å¤§çš„ç»æµä½“ä¹‹ä¸€ã€‚ä¸­å›½çš„å†å²å¯ä»¥è¿½æº¯åˆ°å…¬å…ƒå‰5000å¹´å·¦å³ï¼Œä»å¤è‡³ä»Šéƒ½æœ‰å…¶ç‹¬ç‰¹çš„æ–‡åŒ–å’Œè¯­è¨€ä¼ æ‰¿è€…ã€‚

```

## 3.æ¨¡å‹

### 3.1 Tokenizer

LLMåˆ†è¯å™¨çš„æ„å»ºæ–¹å¼æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯è‡ªå·±æ„é€ è¯è¡¨ï¼Œè®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨ï¼›å¦ä¸€ç§æ˜¯é€‰æ‹©å¼€æºæ¨¡å‹è®­ç»ƒå¥½çš„åˆ†è¯å™¨ã€‚

æœ¬é¡¹ç›®ä¸ºäº†æ–¹ä¾¿ï¼Œä»ä¼˜ç§€çš„å¼€æºé¡¹ç›®ä¸­é€‰æ‹©è¯è¡¨ï¼Œè€ƒè™‘åˆ°è®­ç»ƒçš„æ¨¡å‹è¾ƒå°ï¼Œä¸”è¯è¡¨å¤§å°å½±å“æ¨¡å‹å¤§å°ï¼Œæ•…ä¼˜å…ˆé€‰æ‹©è¯è¡¨è¾ƒå°çš„å¼€æºé¡¹ç›®ï¼›ç»è¿‡æ¯”è¾ƒï¼Œæœ€ç»ˆé€‰æ‹© [ChatGLM3](https://huggingface.co/THUDM/chatglm3-6b) çš„è¯è¡¨ï¼Œè¯¥è¯è¡¨å¤§å°ä¸º 64798 ã€‚

è‡ªå·±æ„é€ è¯è¡¨æ–¹å¼è§ [tokenizer](tokenizer/)ï¼Œæ‰©å…… LLaMA2çš„32Kè¯è¡¨ä¸º50Kï¼Œå¢åŠ 20Kä¸­æ–‡è¯è¡¨ï¼Œè¯¦ç»†æ‰©å……æ–¹å¼è§[æ–‡æ¡£](./doc/)æˆ–[tokenizer/README.md](./tokenizer/README.md).

æ³¨æ„ï¼šæœ¬é¡¹ç›®ä½¿ç”¨çš„ChatGLM3çš„è¯è¡¨ã€‚

### 3.2 æ¨¡å‹ç»“æ„

æ¨¡å‹ç»“æ„é‡‡ç”¨ç±»Llama2çš„ç»“æ„ï¼Œå…·ä½“åŒ…æ‹¬ï¼šRMSNormï¼ŒRoPEï¼ŒMHAç­‰ï¼›


### 3.3 æ¨¡å‹å°ºå¯¸

å…·ä½“å‚æ•°ç»†èŠ‚å¦‚ä¸‹æ‰€ç¤ºï¼š

| model            | hidden size | intermediate size | n_layers | n_heads | max context length | params | vocab size |
| ---------------- | ----------- | ----------------- | -------- | ------- | ------------------ | ------ | ---------- |
| tiny-llm-16m     | 120   | 384        | 6       | 6          | 512                | 16M     | 64798      |
| tiny-llm-42m     | 288   | 768        | 6       | 6          | 512                | 42M     | 64798      |
| tiny-llm-92m     | 512   | 1024       | 8       | 8          | 1024               | 92M     | 64798      |
| tiny-llm-210m    | 768   | 2048       | 16      | 12         | 1024               | 210M    | 64798      |
| tiny-llm-440m    | 1024  | 2816       | 24      | 16         | 1024               | 440M    | 64798      |
| tiny-llm-1_5b    | 2048  | 5504       | 24      | 16         | 1024               | 1.5B    | 64798      |


### 3.4 æ¨¡å‹è¯„ä¼°

å› è®­ç»ƒæ•°æ®å’Œå¾®è°ƒæ•°æ®ï¼Œå¤§éƒ¨åˆ†éƒ½æ˜¯ä¸­æ–‡æ•°æ®ï¼Œæ‰€ä»¥åœ¨`C-Eval`å’Œ`CMMLU`è¿™ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæ¨¡å‹çš„è¯„ä¼°ï¼›ä½¿ç”¨[OpenCompass](https://github.com/open-compass/opencompass)å·¥å…·ï¼Œè¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œè¯„ä¼°åˆ†æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š

| model            | Type  | C-Eval  |  CMMLU  |
| ---------------- | ----- | ------- | ------- |
| tiny-llm-92m     | Base   | 23.48  | 25.02   |
| tiny-llm-92m     | Chat   | 26.79  | 26.59   |

Baseæ¨¡å‹ï¼Œé‡‡ç”¨è¯„æµ‹æ–¹å¼ ppl æ–¹å¼è¿›è¡Œè¯„æµ‹ï¼›Chatæ¨¡å‹ï¼Œé‡‡ç”¨ gen æ–¹å¼è¯„æµ‹ã€‚å…·ä½“åŒºåˆ«å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![ppl gen](doc/image/ppl_gen.png)

> æ¥æºï¼š[pplå’Œgenæ¨¡å¼æœ‰ä»€ä¹ˆåŒºåˆ«](https://github.com/open-compass/opencompass/discussions/597)

æ³¨æ„ï¼šåªå¯¹å¸¸ç”¨çš„ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„æµ‹ï¼Œåˆ†æ•°è¾ƒä½ï¼Œå…¶ä½™æ¨¡å‹è¯„æµ‹æ„ä¹‰ä¸å¤§ã€‚


## 4.æ¨¡å‹éƒ¨ç½²

### 4.1 ç½‘é¡µDemo

ç½‘é¡µDemoå·²éƒ¨ç½²ï¼Œå¯ä»¥åœ¨å¦‚ä¸‹ç½‘ç«™ä¸Šä½“éªŒï¼š[ModeScope Tiny LLM](https://www.modelscope.cn/studios/wdndev/tiny_llm_92m_demo/summary)

å¦‚æœæƒ³åœ¨æœ¬åœ°è¿è¡Œç½‘é¡µDemoï¼Œæ³¨æ„ä¿®æ”¹ `web_demo.py` æ–‡ä»¶ä¸­æ¨¡å‹çš„è·¯å¾„`model_id`ï¼Œè¾“å…¥å¦‚ä¸‹å‘½ä»¤å³å¯è¿è¡Œï¼š

```shell
streamlit run web_demo.py
```

![web demo](doc/image/web_demo.png)

### 4.2 Transformers

Transfomers æ¡†æ¶éƒ¨ç½²ï¼Œä½äº `demo/infer_chat.py` å’Œ `demo/infer_func.py` æ–‡ä»¶ä¸­ï¼Œå’Œå…¶ä»–LLMè¿è¡Œæ— å¤ªå¤§åŒºåˆ«ï¼Œæ³¨æ„è¾“å…¥çš„æ‹¼æ¥å³å¯ã€‚


### 4.3 FastAPI



### 4.4 vllm

è¯¦ç»†vllméƒ¨ç½²è§ [vllm](vllm/README.md)

å¦‚æœä½¿ç”¨**CUDA 12 ä»¥ä¸Šå’ŒPyTorch 2.1 ä»¥ä¸Š**ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…vLLMã€‚

```shell
pip install vllm==0.4.0
```

å¦åˆ™è¯·å‚è€ƒvLLMå®˜æ–¹çš„[å®‰è£…è¯´æ˜](https://docs.vllm.ai/en/latest/getting_started/installation.html)ã€‚

å®‰è£…å®Œæˆåï¼Œè¿˜éœ€è¦ä»¥ä¸‹æ“ä½œ~

1. æŠŠ `vllm/tinyllm.py` æ–‡ä»¶å¤åˆ¶åˆ°envç¯å¢ƒå¯¹åº”çš„ `vllm/model_executor/models` ç›®å½•ä¸‹ã€‚
2. ç„¶ååœ¨vllm/model_executor/models/\_\_init\_\_.pyæ–‡ä»¶å¢åŠ ä¸€è¡Œä»£ç 

```shell
"TinyllmForCausalLM": ("tinyllm", "TinyllmForCausalLM"),
```

> ç”±äºæ¨¡å‹ç»“æ„æ˜¯è‡ªå·±å®šä¹‰çš„ï¼Œvllmå®˜æ–¹æœªå®ç°ï¼Œéœ€è¦è‡ªå·±æ‰‹åŠ¨åŠ å…¥

### 4.5 llama.cpp

è¯¦ç»† llama.cpp éƒ¨ç½²è§ [llama.cpp](llama.cpp/README.md)

Tiny LLM 92M æ¨¡å‹å·²æ”¯æŒ llama.cpp C++ æ¨ç†æ¡†æ¶ï¼Œå»ºè®®åœ¨ linux ç¯å¢ƒä¸‹æµ‹è¯•ï¼Œwindowsæ•ˆæœä¸å¥½ï¼›

æ‰€æ”¯æŒ llama.cpp ä¸ºè‡ªå·±ä¿®æ”¹çš„ç‰ˆæœ¬ï¼Œä»“åº“é“¾æ¥ä¸ºï¼š [llama.cpp.tinyllm](https://github.com/wdndev/llama.cpp.tinyllm)
